apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
    internal.serving.kserve.io/localmodel-pvc-name: llama-3-1-70b-instruct-gptq-w8a8-mlx-a100
    internal.serving.kserve.io/localmodel-sourceuri: bri:mlx:tenancy:llm-public:model:llama-3-1-70b:revision:instruct-gptq-w8a8
    prometheus.kserve.io/path: /metrics
    prometheus.kserve.io/port: "8080"
    serving.kserve.io/deploymentMode: RawDeployment
    sidecar.opentelemetry.io/inject: "true"
  labels:
    app: isvc.autoscaling-llama-3-1-70b-instruct-vllm-predictor
    component: predictor
    internal.serving.kserve.io/localmodel: llama-3-1-70b-instruct-gptq-w8a8-mlx
    serving.kserve.io/inferenceservice: autoscaling-llama-3-1-70b-instruct-vllm
  name: autoscaling-llama-3-1-70b-instruct-vllm-predictor
  namespace: s-dsplatform
spec:
  progressDeadlineSeconds: 600
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: isvc.autoscaling-llama-3-1-70b-instruct-vllm-predictor
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        internal.serving.kserve.io/localmodel-pvc-name: llama-3-1-70b-instruct-gptq-w8a8-mlx-a100
        internal.serving.kserve.io/localmodel-sourceuri: bri:mlx:tenancy:llm-public:model:llama-3-1-70b:revision:instruct-gptq-w8a8
        prometheus.kserve.io/path: /metrics
        prometheus.kserve.io/port: "8080"
        serving.kserve.io/deploymentMode: RawDeployment
        sidecar.opentelemetry.io/inject: "true"
      creationTimestamp: null
      labels:
        app: isvc.autoscaling-llama-3-1-70b-instruct-vllm-predictor
        component: predictor
        internal.serving.kserve.io/localmodel: llama-3-1-70b-instruct-gptq-w8a8-mlx
        serving.kserve.io/inferenceservice: autoscaling-llama-3-1-70b-instruct-vllm
      name: autoscaling-llama-3-1-70b-instruct-vllm-predictor
      namespace: s-dsplatform
    spec:
      containers:
        - args:
            - --model
            - /mnt/models
            - --port
            - "8080"
            - --served-model-name
            - autoscaling-llama-3-1-70b-instruct-vllm
            - --model=/mnt/models
            - --port=8080
            - --served-model-name=llama/meta-llama-3-8b-instruct
            - --load-format=safetensors
            - --kv-cache-dtype=auto
            - --guided-decoding-backend=outlines
            - --tensor-parallel-size=1
            - --gpu-memory-utilization=0.99
            - --max-num-batched-tokens=2048
            - --max-model-len=2048
            - --enable-auto-tool-choice
            - --tool-call-parser=llama3_json
            - --dtype=float16
            # - --chat-template=/mnt/models/tool_chat_template_llama3.1_json.jinja
          image: docker.io/vllm/vllm-openai:v0.6.4
          imagePullPolicy: IfNotPresent
          name: kserve-container
          ports:
            - containerPort: 8080
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 8080
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "4"
              memory: 16Gi
              nvidia.com/gpu: "1"
            requests:
              cpu: "4"
              memory: 8Gi
              nvidia.com/gpu: "1"
---

apiVersion: v1
kind: Service
metadata:
  annotations:
    ds.bloomberg.com/gpuSubtype: bloomberg.com/gpu-a100
    identities.bloomberg.com/initContainerNames: storage-initializer
    internal.serving.kserve.io/localmodel-pvc-name: llama-3-1-70b-instruct-gptq-w8a8-mlx-a100
    internal.serving.kserve.io/localmodel-sourceuri: bri:mlx:tenancy:llm-public:model:llama-3-1-70b:revision:instruct-gptq-w8a8
    internal.serving.kserve.io/storage-initializer-sourceuri: bri:mlx:tenancy:llm-public:model:llama-3-1-70b:revision:instruct-gptq-w8a8
    prometheus.kserve.io/path: /metrics
    prometheus.kserve.io/port: "8080"
    serving.kserve.io/deploymentMode: RawDeployment
    sidecar.opentelemetry.io/inject: "true"
    yas.bloomberg.com/mlx-bri-predictor: bri:mlx:tenancy:llm-public:model:llama-3-1-70b:revision:instruct-gptq-w8a8
  labels:
    app: isvc.autoscaling-llama-3-1-70b-instruct-vllm-predictor
    component: predictor
    internal.serving.kserve.io/localmodel: llama-3-1-70b-instruct-gptq-w8a8-mlx
    serving.kserve.io/inferenceservice: autoscaling-llama-3-1-70b-instruct-vllm
  name: autoscaling-llama-3-1-70b-instruct-vllm-predictor
  namespace: s-dsplatform
spec:
  selector:
    app: isvc.autoscaling-llama-3-1-70b-instruct-vllm-predictor
  ports:
    - port: 80
      protocol: TCP
      targetPort: 8080
  sessionAffinity: None
  type: ClusterIP
